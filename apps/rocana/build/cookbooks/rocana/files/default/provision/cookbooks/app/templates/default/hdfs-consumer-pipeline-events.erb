###################################################################################################
# Name: pipeline._<config name>.kafka.zookeeper.quorum
# Required: Yes
#
# For each pipeline configuration named in the `pipeline` property, the
# following properties are valid. Required properties are indicated as such.
# Each property in a pipeline configuration is prefixed by the name of the
# pipeline configuration. In this sample, we're creating an `events` pipeline
# configuration.

# A comma separated list of <hostname>:<port> of zookeeper servers where the
# Kafka brokers are registered. It's also possible to specify an optional root
# path in ZooKeeper after the port. This is useful when you're using a shared
# ZooKeeper quorum with other services.
#
# Ex.
pipeline.events.kafka.zookeeper.quorum = <%=@node['zookeeper']['url']%>

###################################################################################################
# Name: pipeline.<config name>.topic
# Required: Yes
#
# The name of the Kafka topic from which to consume events.
#
# Ex.
pipeline.events.topic = <%=@node['rocana']['kafka_topic_name']%>

###################################################################################################
# Name: pipeline.<config name>.repo
# Required: Yes
#
# The location of the dataset repository containing this pipeline's dataset.
#
# Ex. Look for datasets in HDFS under /datasets/rocana.
pipeline.events.repo = repo:hdfs://<%=@node['hdfs']['url']%>/datasets/rocana

###################################################################################################
# Name: pipeline.<config name>.namespace
# Required: No
# Default: default
#
# The namespace of the dataset repository containing this pipeline's dataset.
#
# Ex. Look for datasets in the rocana namespace
# pipeline.events.namespace = rocana
###################################################################################################
# Name: pipeline.<config name>.dataset
# Required: Yes
#
# The name of the dataset to which events in this pipeline should be written.
# This dataset must already exist. To create a dataset, see the rocana-data command.
#
# Ex.
pipeline.events.dataset = <%=@node['rocana']['kafka_topic_name']%>

###################################################################################################
# Name: pipeline.<config name>.consumer.group.id
# Required: Yes
#
# The group name of this consumer. When multiple consumers are part of the same
# group, they split up the work of taking the data. This gives you greater
# parallelism, but means there's no global ordering of events.
#
# Ex.
pipeline.events.consumer.group.id = hdfs-consumer

###################################################################################################
# Name: pipeline.<config name>.key.cache.size
# Required: No
#
# The minimum number of Kafka message keys per topic-partition to cache. If a cached key is seen
# again in the same partition the message is a duplicate, and it isn't processed. A value of 0
# disables de-duplication and stores all messages, regardless of their key.
#
# Ex.
# pipeline.events.key.cache.size = 8192
##################################################################################################
# Name: pipeline.<config name>.username
# Required: Yes (in a kerberos environment)
#
# When the consumer is run in a kerberos secured environment each pipeline can write as a different
# user. Alternatively, the user can be arbitrarily overridden when running outside of a kerberos
# environment.
#
# Ex.
# pipeline.events.username = hdfs-user1@EXAMPLE.COM
###################################################################################################
# Name: pipeline.<config name>.key.serializer.class
# Name: pipeline.<config name>.serializer.class
# Required: No
#
# Advanced. If the Rocana-Agent is configured to use the JSON encoding, then you need to configure
# an additional json-events pipeline. This pipeline needs to include the following serializer parameters.
#
# Ex.
# pipeline.json-events.key.serializer.class = kafka.serializer.DefaultDecoder
# pipeline.json-events.serializer.class = com.rocana.kafka.JsonEventDecoder
###################################################################################################
# Name: pipeline.<config name>.threads
# Required: Yes
#
# The number of pipelines created with this configuration. For each thread there
# will be a separate Kafka stream/HDFS stream. More threads means more work is
# performed in parallel. For a dedicated machine, you probably want at least 1
# worker for each physical cpu core.
#
# Ex.
pipeline.events.threads = 1

###################################################################################################
# Name: pipeline.<config name>.dead-letter.mode
# Required: Yes
#
# Dead letter mode determines what the system will do with improperly formatted events. The possible
# values are "queue" or "drop". If "queue" is selected, then you must also specify the dead letter
# topic, kafka brokers, and serializer class. Improperly formatted events will be placed on
# the specified kafka topic.
#
# If "drop" is specified, the improperly formatted events will just be dropped.
#
# Ex.
pipeline.events.dead-letter.mode = drop

###################################################################################################
# Name: pipeline.<config name>.dead-letter.topic
# Required: Yes (if dead letter mode == queue)
#
# If dead letter mode has been set to "queue", then this will specify which kafka topic these
# events should be placed on.
#
# Ex.
# pipeline.events.dead-letter.topic = bad_events
###################################################################################################
# Name: pipeline.<config name>.dead-letter.producer.kafka.brokers
# Required: Yes (if dead letter mode == queue)
#
# A comma separated list of <hostname>:<port> of kafka brokers to use.
#
# Ex.
# pipeline.events.dead-letter.producer.kafka.brokers = kafka1.rocana.com:9092,kafka2.rocana.com:9092
###################################################################################################
# Name: pipeline.<config name>.dead-letter.producer.serializer.class
# Required: Yes (if dead letter mode == queue)
#
# You must configure how the events are to be encoded.
#
# Ex.
# pipeline.events.dead-letter.producer.serializer.class = kafka.serializer.DefaultEncoder
###################################################################################################
# Name: pipeline.<config name>.writer.batch-time-ms
# Required: No
#
# The frequency with which received events are written to HDFS, in milliseconds.
# The recommended lower bound and default is 1s (1000). Note that files in HDFS are only
# rolled based on commit time, not batches.
#
# Ex.
# pipeline.events.writer.batch-time-ms = 1000
###################################################################################################
# Name: pipeline.<config name>.writer.batch-size
# Required: No
#
# The maximum number of events the consumer will consume before writing the batch of events to HDFS.
# The recommended lower bound and default is 1000. Note that files in HDFS are only
# rolled based on commit time, not batches.
#
# Ex.
# pipeline.events.writer.batch-size = 1000
###################################################################################################
# Name: pipeline.<config name>.writer.commit-time-ms
# Required: No
#
# The frequency with which files in HDFS are rolled, in milliseconds. The recommended lower bound and
# default is 10m (600000) and the interval must be >= 10s (10000).
#
# Ex.
# pipeline.events.writer.commit-time-ms = 600000
###################################################################################################
# Name: pipeline.<config name>.writer.drop-event-types
# Required: No
#
# A comma seperated list of event type IDs that should be dropped by the writer.
# Defaults to "107" (host metrics). To keep all event types, assign this to an empty value.
#
# Ex.
# pipeline.events.writer.drop-event-types = 107
