# A sample configuration file for configuring the Rocana agent

[kafka]
# Brokers - Comma separated list of Kafka broker hosts where events should be published.
# This is a required setting, and if it is missing or invalid the agent will fail to start.
# An example setting which uses two brokers and specified ports
# Brokers=kafkahost01.mydomain.com:9092,kafkahost02.mydomain.com:9092
Brokers=localhost:9092

# RequiredAcks - The level of acknowledgement you wish to receive before the message
# is to be considered delivered. There is a performance/reliability trade-off inherent
# in choosing a value for this setting.  Defaults to `WaitForLocal`.
#
# Accepted Values:
#   NoResponse - The producer never waits for an acknowledgement from the broker. This
#                option provides the lowest latency but the weakest durability guarantees
#                (some data will be lost when a server fails).
#
#   WaitForLocal - The producer gets an acknowledgement after the leader replica has received
#                the data. This option provides better durability as the client waits until
#                the server acknowledges the request as successful (only messages that were
#                written to the now-dead leader but not yet replicated will be lost).
#
#   WaitForAll - The producer gets an acknowledgement after all in-sync replicas have received
#                the data. This option provides the best durability, we guarantee that no
#                messages will be lost as long as at least one in sync replica remains.
# RequiredAcks=WaitForLocal

# Partitioner - Method for partitioning messages amongst sub-topics. Defaults to `Random`.
#
# Accepted Values:
#   Random - Chooses a random partition each time.
#
#   RoundRobin - Walks through the available partitions one at a time.
#
#   Hash - If the key is nil, or fails to encode, then a random partition is chosen.
#                Otherwise the FNV-1a hash of the encoded bytes is used modulus the number
#                of partitions. This ensures that messages with the same key always end up
#                on the same partition.
# Partitioner=Random

# ClientID - a user-specified string sent to Kafka with each request to help trace calls. This
# is an advanced configuration option that should only need to be changed to aid in debugging.
# Defaults to `rocana-agent`.
# ClientId=rocana-agent

# EventsTopic - The name of the Kafka topic that Events should be written to. If `EventEncoding` is
# set to `AVRO`, this can be the `events` topic. If `EventEncoding` is set to `JSON` this must not be
# `events` - Avro and JSON events cannot be sent to the same topic. Defaults to `events`.
# EventsTopic = events

# EventEncoding - The encoder to use when writing messages to Kafka. Defaults to `AVRO`,
# for Avro binary encoding. `JSON` is also supported. If this is set to `JSON`, `EventsTopic`
# must also be changed to `json_events`: Avro and JSON events cannot be sent to the same Kafka topic.
# EventEncoding = AVRO

# BatchSize - The number of records to hold on to before performing a transmission to Kafka.
# Increasing this number will improve throughput as aproximately one RPC call to Kafka
# is performed per batch.  Note that increased batch size also risks an increased volume of
# data loss in the event of an unclean shutdown. Defaults to `500`.
# BatchSize = 500

# RetryWaitMS - The number of milliseconds to wait before retrying to send messages to Kafka
# after they have failed to send on their first attempt.  For any subsequent attempts, the
# amount of time we wait will grow exponentially. Defaults to 250 milliseconds.
# RetryWaitMS = 250

# NumRetries - The number of attempts to make for any given message to be successfully sent to
# Kafka.  After this many attempts the message will be discarded and a warning will be issued.
# The exception to this is messages sourced from durable data sources (eg - tailing/spooling). These
# messages will be retried until either they are either successfully or the agent is shutdown. Defaults to `4`.
# NumRetries = 4

# BatchTimeoutMS - The number of milliseconds after the last batch flush when we should perform
# another flush even if we have not read `BatchSize` number of records. Defaults to 1000 milliseconds.
# BatchTimeoutMS = 1000

[BackingStore]
# DataLocation - The directory where the agent will store checkpoint information about
# files that are being tailed and spooled. After processing a message from a file, the agent will
# write the byte offset where the message ended. In the event of a shutdown this offset will
# be read in and used to determine where to continue reading the file from.  The user running
# the agent will need write permission to this directory.
# DataLocation=/var/lib/rocana-agent/tailing-checkpoints/

[metrics]
# logEnabled - Whether or not to periodically info log key metrics about the agent. Defaults
# to true.
# logEnabled=true

# logIntervalSecs - Number of seconds between metrics between each time we write metrics to the
# info log. Defaults to 60 seconds.
# logIntervalSecs=60

# httpEnabled - Enabling http causes the agent to launch an http server which responds with a
# json struct describing key agent metrics. Defaults to true.
# httpEnabled=true

# httpAddress - Address the http metrics server should listen. Defaults to `0.0.0.0`.
# httpAddress=0.0.0.0

# httpPort - Port the http metrics server should be running on. Defaults to `17313`.
# httpPort=17313

# GcEnabled - Whether or not to collect garbage collection metrics. Defaults to true.
# GcEnabled = true

# GcRefreshSecs - Number of seconds between refresh of garbage collection metrics.
# GcRefreshSecs = 10


[syslog]
# MessageFormat - A regular expression to use to decode syslog messages which are not RFC3164 or
# RFC5424-compliant. The syslog server will always try to decode messages using these two standards.
# If a message cannot be decoded with either, the server will use this pattern to extract the following fields
# from the message: `pid`, `process`, `hostname`, `message`, `timestamp`, `severity`, `facility`, `pri`.
# If `pri` is extracted, it will be processed and `severity` and `facility` will be populated in the resulting
# event. If the pattern does not match the fields will not be extracted, but the raw message will be forwarded.

# Note that the regular expression must be contained in double quotes, and all '\' need to be escaped as '\\'.
# See the description of TimeFormat below for a description of how timestamps are handled.
# Defaults to an empty string, which disables custom message handling.
# MessageFormat=""

# TimeFormat - If MessageFormat is set, the `timestamp` field will be parsed using this layout. The layout is
# defined by specifying how the following time value would be encoded: Mon Jan 2 15:04:05 -0700 MST 2006
# If the matched `timestamp` value can not be parsed with this format, or if there is no `timestamp` field
# extracted, then the timestamp will be set to the current time of the local machine. If there is no year
# in the time format it will be set automatically based on the current local time.
# TimeFormat=Jan 02 15:04:05

# TcpEnabled - Whether or not the agent should listen for Syslog events over TCP. Defaults to `true`.
# TcpEnabled=true

# TcpAddress - Address that the agent should listen on for TCP based syslog messages.  Defaults to
# to `0.0.0.0`.
# TcpAddress=0.0.0.0

# TcpPort - Port that the agent should listen on for TCP based syslog messages.  Defaults to `17320`.
# TcpPort=17320

# TcpQueueSize - Number of TCP connections to accept and queue while waiting to be served. Under most
# circumstances the operating system will handle queuing unaccepted connections, so it's not necessary
# to change this from the default. For example, on Linux TCP connection queue depth is defined by `SOMAXCONN`.
# Defaults to `0`.
# TcpQueueSize=0

# TcpConcurrentConnections - Number of TCP syslog connections to serve concurrently.  This should be set 
# to at least the number of servers this agent will be forwarding syslog messages for. Defaults to `16`.
# TcpConcurrentConnections=16

# TcpInitialMsgBufSize - The number of bytes to expect initially when recieving syslog messages over TCP.
# Processing a message will take at least this much, and at most `TcpMaxMsgBufSize` memory. Advanced users
# can increase this value to reduce the number of buffer resizes which occur, or decrease it to reduce memory
# usage. Defaults to `4096`.
# TcpInitialMsgBufSize=4096

# TcpMaxMsgBufSize - The maximum number of bytes to buffer when recieving syslog messages over TCP. This
# prevents malformed messages from consuming excessive memory. Messages larger than this size will be
# split into multiple messages of up to `TcpMaxMsgBufSize`. Defaults to `65536`
# TcpMaxMsgBufSize=65536

# UdpEnabled - Whether or not the agent should listen for Syslog events over UDP. Defaults to `false`.
# UdpEnabled=false

# UdpAddress - Address that the agent should listen on for UDP based syslog messages.  Defaults to
# to `0.0.0.0`.
# UdpAddress=0.0.0.0

# UdpPort - Port that the agent should listen on for UDP based syslog messages.  Defaults to `17321`.
# UdpPort=17321

# UdpQueueSize - Number of UDP syslog messages to accept and queue in agent memory. If the backlog exceeds
# this number the operating system will be responsible for buffering messages, and it may drop some as necessary.
# On Linux, for example, the maximum buffer size is determined by SO_RECVBUF. Defaults to `4096`.
# UdpQueueSize=4096

# UdpWorkers - Number of concurrent routines which will be created to handle UDP syslog messages. If all
# workers are busy, messages are queued up to a maximum of `UdpQueueSize`. Defaults to `8`.
# UdpWorkers=8

[events]

# Location - The location name where this agent runs. Typically, this is a data center, region,
# or other similar designation. You are free to encode hierarchical information in this value
# by separating components with slashes. Defaults to an empty string.
# Location=nyc-1/zone-2/rack-12

# Host - The host on which this agent runs. If no value is specified or an empty string is
# specified for this setting, it will default to the hostname according to the OS.
# Host=host02.mydomain.com

# Service - The name of the service producing event data. Like `Host`, this will be automatically
# discovered in most cases. When receiving events via syslog, the service will be taken from
# the service field of the syslog message.
# Service=rocana-agent


[FileTailing]

# The following settings are to be repeated for every file that is to be tailed, and the order in
# which they appear is significant.  Specifically, each setting after the `Path=...` entry is assumed
# to be the setting for that file.  Also note that all settings are required for every file to be
# tailed, even if there is a default you need to have an empty value to signify your desire to use the
# default. If a setting is missing for a specific file the agent will not be able to determine which
# setting corresponds to which file and will exit with an error.

# Path - The path to a file to be tailed. The user running the agent will need read permission on this
# file as well as its containing directory.
# Path=/tmp/mmap-test-data.log

# FieldRegex - A regular expression containing named groupings that is used to extract named fields from
# messages in the file.  If the overall regular expression matches the message, each named group from the
# regex will be used as a key/value pair in the resulting Event's attribute map.  Note that the regular
# expressions need to be contained in double quotes, and all '\' need to be escaped as '\\'.  Also note that
# if you specify a grouping named 'location', 'host', or 'service' and there is a match, the value in the
# grouping will override the corresponding Event's attributes. See the description of TimeFormat below for
# a description of how similar behavior can be achieved with timestamps.
# FieldRegex="^(?P<timestamp>[a-zA-Z]{3} [0-9: ]+) (?P<host>[a-z.]*).*USER=(?P<location>[a-z].*) ; COMMAND=(?P<service>[/a-z \\-].*)"

# FileNameRegex - A regular expression containing named groupings that is used to extract named fields from
# the path of the file. The same rules apply as for FieldRegex around escaping and grouping names. Groupings
# from FieldRegex will override groupings with the same name from FileNameRegex. Setting this to an empty
# string does not do any processing of the filename.
# FileNameRegex=""

# TimeFormat - If the regular expression matches, and there is a grouping named 'timestamp' we can use this
# value to set the time stamp of a message.  In order to parse the value, we need a TimeFormat that describes
# the layout of the timestamp.  The layout is defined by specifying how the following time value would be
# encoded if it were in this file: Mon Jan 2 15:04:05 -0700 MST 2006
# If the matched `timestamp` value can not be parsed with this format the Event's timestamp field will not be
# modified but the value will still be included in the attributes mapping.  If there is no time zone or year
# in the time format the default of the system where the agent is running will be used.
# TimeFormat=Jan 02 15:04:05

# ShouldStartAtEnd - Where the tailer should start reading a file if it does not have a previous byte offset
# record for the file. This situation comes about either because it is the first time the file is being tailed
# or if the checkpoint data was lost. If true, the tailer will start reading from the end of the file, but if
# false the tailer will read all existing data in the file.
# ShouldStartAtEnd=false

# RecordSep - String that indicates the termination of a record. This string will be included with the body of
# the message.  If enclosed in double quotes you can signify new line and tab chars as "\n" or "\t". Note that
# if `RecordSepRegex` is not empty, it will override this configuration.
# RecordSep="\n"

# RecordSepRegex - A regular expression that indicates the termination of a record. The string matching this pattern
# will be included with the body of the message.  If enclosed in double quotes you can signify new line and tab chars
# as "\n" or "\t". If this parameter is empty, `RecordSep` will be used instead. Using `RecordSep` is faster
# and more efficient when splitting on a fixed string.
# RecordSepRegex=""

# MaxMsgSize - As a safe guard against malformed data or misconfigured record separator, you can specify a maximum
# number of bytes to hold in memory before which the message should be truncated. This prevents the agent from dying
# of memory exhaustion in the event of a large data volume with no separator(s). If you do not wish to use truncation
# you can disable this feature by using a value of -1.
# MaxMsgSize=65536

# InitialMsgBufSize - Number of bytes per record to expect initially. The agent will use at least this much memory,
# and at most MaxMsgSize, to read the file. A small size  will use less memory for small records, but impact
# performance for large records.
# InitialMsgBufSize=4096

# EventTypeId - The event type id for the events parsed from this file. This can be set so that events are
# visualized using the correct event type template in the web UI. If you leave this setting blank (i.e. EventTypeId= )
# then the default type id for a generic tailed file record will be used. You can override this value using a group
# named 'event_type_id' inside of the FieldRegex setting.
# EventTypeId=

# Location - The location name where this file lives. Typically, this is a data center, region,
# or other similar designation. You are free to encode hierarchical information in this value
# by separating components with slashes.  You can override this value using a group named
# 'location' inside of the FieldRegex setting.
# Location=nyc-1/zone-2/rack-12

# Host - The host on which this file exists. You can override this value using a group named
# 'host' inside of the FieldRegex setting. If no value is specified or an empty string is
# specified for this setting, it will default to the hostname according to the OS.
# Host=host02.mydomain.com

# Service - The name of the service producing this file. You can override this value using
# a group named 'service' inside of the FieldRegex setting.
# Service=

# BatchSize - The number of records to read from the file before performing a transmission to
# Kafka. Increasing this number will limit IO performed by the agent as only one checkpoint is
# performed per batch. This should not be larger than the Kafka `BatchSize`.  Note that increased
# batch size also risks duplicate records as a recovering agent will retransmit from the last
# checkpointed offset.
# BatchSize=10

# BatchTimeoutMS - The number of milliseconds after the last batch flush when we should perform
# another flush even if we have not read `BatchSize` number of records.
# BatchTimeoutMS=500

[DirectorySpool]

# The following settings are to be repeated for every directory that is to be watched, and the order in
# which they appear is significant.  Specifically, each setting after the `Path=...` entry is assumed
# to be the setting for that directory.  Also note that all settings are required for every directory to be
# watched, even if there is a default you need to have an empty value to signify your desire to use the
# default. If a setting is missing for a specific directory the agent will not be able to determine which
# setting corresponds to which directory and will exit with an error.

# Path - The path to a directory to be watched. Files copied into this directory will be read and processed.
# The user running the agent will need read permission on this directory and every parent directory.
# Path=/tmp/test-dir/

# FileNameRegex - A regular expression used to select which files copied to the spooling directory will be
# processed. If a file's whole path (including the spooling directory and any parents) matches this regular
# expression it will be read. Otherwise the file will be ignored. Any named groupings in the expression will
# be extracted from the file path and included as a key/value pair in the result Event's attribute map.
# Note that the regular expression needs to be contained in double quotes, and all '\' need to be escaped
# as '\\'.  Also note that if you specify a grouping named 'location', 'host', or 'service' and there is
# match, the value in the grouping will override the corresponding Event's attributes. See the description
# of TimeFormat below for a description of how similar behavior can be achieved with timestamps. Note that
# groups from the file name are overwritten by a group of the same name from the FieldRegex.
# FileNameRegex=""

# FieldRegex - A regular expression containing named groupings that is used to extract named fields from
# messages in the files copied to the directory.  If the overall regular expression matches the message,
# each named group from the regex will be used as a key/value pair in the resulting Event's attribute map.
# Note that the regular expressions need to be contained in double quotes, and all '\' need to be escaped
# as '\\'.  Also note that if you specify a grouping named 'location', 'host', or 'service' and there is
# match, the value in the grouping will override the corresponding Event's attributes. See the description
# of TimeFormat below for a description of how similar behavior can be achieved with timestamps.
# FieldRegex="^(?P<timestamp>[a-zA-Z]{3} [0-9: ]+) (?P<host>[a-z.]*).*USER=(?P<location>[a-z].*) ; COMMAND=(?P<service>[/a-z \\-].*)"

# TimeFormat - If the regular expression matches, and there is a grouping named 'timestamp' we can use this
# value to set the time stamp of a message.  In order to parse the value, we need a TimeFormat that describes
# the layout of the timestamp.  The layout is defined by specifying how the following time value would be
# encoded if it were in this file: Mon Jan 2 15:04:05 -0700 MST 2006
# If the matched `timestamp` value can not be parsed with this format the Event's timestamp field will not be
# modified but the value will still be included in the attributes mapping.  If there is no time zone or year
# in the time format the default of the system where the agent is running will be used.
# TimeFormat=Jan 02 15:04:05

# IngestExisting - Whether the agent should pick up new files it finds in the directory when it starts. If
# true, on startup the agent will find and process any files created since it last shutdown. If false, these
# files will be ignored and the agent will immediately start watching for new files to be created.  Note
# that the agent will always finish processing a file created before it shutdown - if reading the file was
# interrupted by shutdown, it will continue on startup.
# IngestExisting=true

# DoneDirectory - The directory to move files to after they've been processed. Files which are not processed
# because of FileNameRegex or IngestExisting will not be moved. This cannot be the same as Path.
# This directory must exist and be writable by the user running the agent.
# DoneDirectory=/tmp/spool-done/

# RecordSep - String that indicates the termination of a record. This string will be included with the body of
# the message.  If enclosed in double quotes you can signify new line and tab chars as "\n" or "\t". Note that
# when a file in the directory does not end with RecordSep, trailing data (the data between the last
# RecordSep and the end of the file) will still always be processed as a record.
# RecordSep="\n"

# RecordSepRegex - A regular expression that indicates the termination of a record. The string matching this pattern
# will be included with the body of the message.  If enclosed in double quotes you can signify new line and tab chars
# as "\n" or "\t". If this parameter is empty, `RecordSep` will be used instead. Using `RecordSep` is faster
# and more efficient when splitting on a fixed string.
# RecordSepRegex=""

# MaxMsgSize - As a safe guard against malformed data or misconfigured record separator, you can specify a maximum
# number of bytes to hold in memory before which the message should be truncated. This prevents the agent from dying
# of memory exhaustion in the event of a large data volume with no separator(s). If you do not wish to use truncation
# you can disable this feature by using a value of -1.
# MaxMsgSize=65536

# InitialMsgBufSize - Number of bytes per record to expect initially. The agent will use at least this much memory,
# and at most MaxMsgSize, to read the file. A small size  will use less memory for small records, but impact
# performance for large records.
# InitialMsgBufSize=4096

# EventTypeId - The event type id for the events parsed from this directory. This can be set so that events are
# visualized using the correct event type template in the web UI. If you leave this setting blank (i.e. EventTypeId= )
# then the default type id for a generic tailed file record will be used. You can override this value using a group
# named 'event_type_id' inside of the FieldRegex setting.
# EventTypeId=

# Location - The location name where this file lives. Typically, this is a data center, region,
# or other similar designation. You are free to encode hierarchical information in this value
# by separating components with slashes.  You can override this value using a group named
# 'location' inside of the FieldRegex setting.
# Location=nyc-1/zone-2/rack-12

# Host - The host on which this file exists. You can override this value using a group named
# 'host' inside of the FieldRegex setting. If no value is specified or an empty string is
# specified for this setting, it will default to the hostname according to the OS.
# Host=host02.mydomain.com

# Service - The name of the service producing this file. You can override this value using
# a group named 'service' inside of the FieldRegex setting.
# Service=

# BatchSize - The number of records to read from the file before performing a transmission to
# Kafka. Increasing this number will limit IO performed by the agent as only one checkpoint is
# performed per batch. This value should be larger than the Kafka `BatchSize`. Note that
# increased batch size also risks duplicate records as a recovering agent will retransmit
# from the last checkpointed offset.
# BatchSize=10

[HostMetrics]
# Enabled - Whether or not we should inspect this host and collect metrics about its current
# status. Defaults to `true`.
# Enabled=true

# CollectIntervalSecs - The frequency (in seconds) with which we should be inspecting the host and generating
# metrics about its current status. Defaults to `5`.
# CollectIntervalSecs=5

# Location - The location where the agent is running. Typically, this is a data center, region,
# or other similar designation. You are free to encode hierarchical information in this value
# by separating components with slashes.  Defaults to an empty string.
# Location=nyc-1/zone-2/rack-12

# Host - The hostname where the agent is running. If no value is specified or an empty string is
# specified for this setting, it will default to the hostname according to the OS.
# Host=host02.mydomain.com

# Service - The name of the service for the host. Defaults to an empty string.
# Service=webpool

# CollectMem - Whether or not to collect metrics on system memory. Enabled by default.
# CollectMem = true

# CollectIO - Whether or not to collect metrics on system IO.  Enabled by default.
# CollectIO = true

# CollectCPU - Whether or not to collect metrics on system processors. Enabled by default.
# CollectCPU = true

# CollectFS - Whether or not collect metrics on file systems. Enabled by default.
# CollectFS = true

# CollectCPUIntervalMS - The time between counter samples when collecting CPU metrics, in milliseconds.
# CPU usage is reported as an average over this time span, collected every `CollectIntervalSecs` seconds.
# This must be less than `CollectIntervalSecs`, or some samples will not be collected. Defaults to `1000`.
# CollectCPUIntervalMS = 1000

# CollectIOIntervalMS - The time between counter samples when collecting IO metrics, in milliseconds.
# IO metrics are reported as an average over this time span, collected every `CollectIntervalSecs` seconds.
# This must be less than `CollectIntervalSecs`, or some samples will not be collected. Defaults to `1000`.
# CollectIOIntervalMS = 1000

[Profiler]
# The agent provides a diagnostic endpoint accessible via HTTP for use
# in diagnosing performance related issues. Most users will never access this
# feature directly, but Rocana support may utilize it in providing information
# on performance issues.

# Enabled - Whether or not the diagnostic profiling endpoint is enabled.
# Enabled by default.
# Enabled = true

# Address - Address the HTTP service should be listening on. Defaults to `0.0.0.0`
# Address = 127.0.0.1

# Port - Port the HTTP service should be listening on. Defaults to 17323.
# Port = 17323
