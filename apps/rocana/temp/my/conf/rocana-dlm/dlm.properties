#
# Copyright (c) 2015 Rocana. All rights reserved.
#
# This is a sample Data Lifecycle Management configuration file. For the full documentation,
# see the Rocana Reference Guide.
#
###################################################################################################
# Name: kerberos.principal
# Required: Yes (in a kerberos environment)
#
# HDFS only: When running in a kerberos secured environment, you must specify an hdfs superuser
# principal and keytab that can be used to proxy the users specified for each job.
#
# Ex.
# kerberos.principal = hdfs@EXAMPLE.COM
#
###################################################################################################
# Name: kerberos.principal.keytab
# Required: Yes (in a kerberos environment)
#
# HDFS only: If running in a kerberos secured environment you need to specify the location of
# the keytab for the superuser to authenticate with.
#
# Ex.
# kerberos.principal.keytab = /opt/rocana/rocana.keytab
#
###################################################################################################
# Name: kerberos.impala.principal
# Required: Yes (in a kerberos environment)
#
# The Kerberos principal used to start Impala on impala.host. It's not necessary to provide a keytab
# for this principal. This principal must include the hostname, which should be impala.host. If you
# leave the hostname set to "_HOST", it will automatically be replaced with the fully qualified
# domain name of the Impala server.
#
# Ex.
# kerberos.impala.principal = impala/_HOST@EXAMPLE.COM
#
###################################################################################################
# Name: impala.host
# Required: Yes
#
# The host that will be used to invalidate pruned Impala data.
#
# Ex.
# impala.host = cdh5-1.rocana.com
#
###################################################################################################
# Name: impala.port
# Required: No
#
# Impala SQL access port, 21050 by default.
#
# Ex.
# impala.port = 21050
#
###################################################################################################
# Name: temp.repo
# Required: Yes
#
# An HDFS repository where temporary datasets will be created for compaction jobs. It *must*
# be a plain (non-Hive, or other) HDFS repository, and must be writable by the user (or principal)
# that runs the DLM service.
#
# Ex.
# temp.repo = repo:hdfs://namenode-host:8020/datasets/rocana
#
###################################################################################################
# Name: temp.namespace
# Required: No
# Default: default
#
# The namespace inside of the temporary repo used to create temporary datasets.
# Only applicable to hdfs repos.
#
# Ex. Create temporary datasets inside the rocana namespace
# temp.namespace = rocana
#
###################################################################################################
# Name: zookeeper.quorum
# Required: Yes
#
# A comma separated list of <hostname>:<port> of zookeeper servers where an exclusive lock will
# be placed to prevent multiple DLM daemons from running at the same time. It's also possible to
# specify an optional root path in ZooKeeper after the port. This is useful when you're using a
# shared ZooKeeper quorum with other services.
#
# Ex.
# zookeeper.quorum = zk-server-1:2181,zk-server-2:2181,zk-server-3:2181
#
###################################################################################################
# A comma separated list of dataset pruner configuration names.
#
# This property only declares the pruner configuration name. Once declared,
# the pruner configuration properties must be specified. See later in this
# file for an example.
#
# Name: prune
# Required: Yes
#
# Ex. A single pruner called solr.
# prune = solr
#
# Ex. Two pruners called solr and hdfs
# prune = solr, hdfs
#
###################################################################################################
# Name: prune.<config name>.repo
# Required: Yes
#
# The location of the dataset repository containing the dataset to be pruned.
#
# Ex.
# prune.solr.repo = zk1.rocana.com:2181,zk2.rocana.com:2181,zk3.rocana.com:2181/solr
# prune.hdfs.repo = repo:hdfs://namenode-host:8020/datasets/rocana
#
###################################################################################################
# Name: prune.<config name>.namespace
# Required: No
#
# The namespace of the dataset repository containing this job's dataset.
# Only applicable to hdfs repos. Defaults to "default".
#
# Ex. Look for datasets in the rocana namespace
# prune.hdfs.namespace = rocana
#
###################################################################################################
# Name: prune.<config name>.dataset
# Required: Yes
#
# For HDFS: The name of the dataset to be pruned.
# For Solr: The name of the solr core to be pruned.
# This dataset must already exist. To create a dataset, see the rocana-data command.
#
# Ex
# prune.solr.dataset = events
# prune.hdfs.dataset = events
#
###################################################################################################
# Name: prune.<config name>.type
# Required: Yes
#
# The type of repo/dataset this pruner operates on. One of 'solr' or 'hdfs'.
#
# Ex
# prune.solr.type = solr
# prune.hdfs.type = hdfs
#
###################################################################################################
# Name: prune.<config name>.schedule
# Required: Yes
#
# The schedule to run this pruner on. Standard Unix cron format (without Year).
# Field order:
#   minutes, hours, days of month, months, days of week
# Note that one of the 'days of month' or 'days of week' must be '*'
# For more information see: https://en.wikipedia.org/wiki/Cron#Format
#
# Ex
# prune.solr.schedule = 0 * * * *
#
###################################################################################################
# Name: prune.<config name>.username
# Required: No
#
# HDFS only: When the pruner is run in a kerberos secured environment each job can write as a
# different user. Alternatively, the user can be arbitrarily overridden when running outside
# of a kerberos environment.
#
# Ex.
# prune.hdfs.username = hdfs-user1@EXAMPLE.COM
#
###################################################################################################
# Name: prune.<config name>.jaas.module.name
# Required: Yes (in a kerberos environment)
#
# Solr only: When running in a kerberos secured environment you need to update the
# conf/jaas-client.conf file with your keytab information.  Add the jaas.module.name property
# below specifying the module you would like to use in the jaas-client.conf file. Finally, you
# need to uncomment the line in conf/commands.d/rocana-dlm referencing the jaas-client.conf.
#
# Ex.
# prune.solr.jaas.module.name = SOLR
#
###################################################################################################
# Name: prune.<config name>.retain-days
# Required: Yes
#
# The number of full days worth of data to retain in addition to the current partial
# day (since UTC midnight). Anything older is subject to be pruned.
#
# Ex. Retain 10 years
# prune.solr.retain-days = 3650
#
###################################################################################################
# A comma separated list of dataset compactor configuration names.
#
# This property only declares the compactor configuration name. Once declared,
# the compactor configuration properties must be specified. See later in this
# file for an example.
#
# Name: compact
# Required: Yes
#
# Ex. A single compactor called hdfs.
# compact = hdfs
#
# Ex. Two compactors called hdfs and hdfs-json
# compact = hdfs, hdfs-json
#
###################################################################################################
# Name: compact.<config name>.repo
# Required: Yes
#
# The location of the dataset repository containing the dataset to be compacted.
#
# Ex.
# compact.hdfs.repo = repo:hdfs://namenode-host:8020/datasets/rocana
#
###################################################################################################
# Name: compact.<config name>.dataset
# Required: Yes
#
# The name of the dataset to be compacted. This dataset must already exist.
# To create a dataset, see the rocana-data command.
#
# Ex
# compact.hdfs.dataset = events
#
###################################################################################################
# Name: compact.<config name>.type
# Required: Yes
#
# The type of repo/dataset this compactor operates on. One of 'solr' or 'hdfs'.
#
# Ex
# compact.hdfs.type = hdfs
#
###################################################################################################
# Name: compact.<config name>.schedule
# Required: Yes
#
# The schedule to run this compactor on. Standard Unix cron format (without Year).
# Field order:
#   minutes, hours, days of month, months, days of week
# Note that one of the 'days of month' or 'days of week' must be '*'
# For more information see: https://en.wikipedia.org/wiki/Cron#Format
#
# Ex
# compact.solr.schedule = 0 * * * *
#
###################################################################################################
# Name: compact.<config name>.username
# Required: No
#
# HDFS only: When the compactor is run in a kerberos secured environment each job can write as a
# different user. Alternatively, the user can be arbitrarily overridden when running outside
# of a kerberos environment.
#
# Ex.
# compact.hdfs.username = hdfs-user1@EXAMPLE.COM
#
###################################################################################################
# Name: compact.<config name>.max-files-per-run
# Required: No
# Default: 100
#
# The maximum number of files in a single partition to compact in one compaction run. Remaining
# files will be left as-is, to be compacted in future runs.
#
# Ex.
# compact.hdfs.max-files-per-run = 100
#
###################################################################################################
# Name: compact.<config name>.max-file-size-mb
# Required: No
# Default: 256
#
# The maximum target size of individual compacted files. Many smaller files will be combined
# into larger files that are at max this size, resulting in one or more larger files
# that are 'max-file-size-mb' or smaller.
#
# Ex.
# compact.hdfs.max-file-size-mb = 256
#
###################################################################################################
# Name: metrics.log-report-frequency
# Required: Yes
#
# The frequency at which metrics are output to the dlm logs, in seconds. If this
# value is 0, then there will not be any metrics output to the logs.
#
# Ex.
metrics.log-report-frequency = 300
#
###################################################################################################
# Name: admin.server.bind-address
# Required: No
# Default: 0.0.0.0
#
# The IP or hostname to which the admin HTTP service should bind. If set to
# 0.0.0.0 (wildcard), the server will bind to all available interfaces.
#
# Ex.
# admin.server.bind-address = localhost
#
# Ex.
# admin.server.bind-address = dlm1234.rocana.com
#
###################################################################################################
# Name: admin.server.port
# Required: Yes
# Default: 17316
#
# The port to which the admin HTTP service should bind.
#
# Ex.
admin.server.port = 17316
#
###################################################################################################
