#
# Copyright (c) 2015 Rocana. All rights reserved.
#
# This is a sample Data Lifecycle Management configuration file. For the full documentation,
# see the Rocana Reference Guide.
#
###################################################################################################
# Name: kerberos.principal
# Required: Yes (in a kerberos environment)
#
# When the consumer is run in a kerberos secured environment, you must specify a rocana-search
# principal and keytab that can be used to authenticate with Hadoop kerberos services.
#
# We recommend that you specify the principal as user/hostname@REALM, rather than just
# user@REALM.
#
# Ex.
# kerberos.principal = rocana/server.example.com@EXAMPLE.COM
###################################################################################################
# Name: kerberos.principal.keytab
# Required: Yes (in a kerberos environment)
#
# If running in a kerberos secured environment you need to specify the location of the keytab for the
# rocana-search principal (e.g. rocana/<hostname>) to authenticate with.
#
# Ex.
# kerberos.principal.keytab = /opt/rocana/rocana.keytab
###################################################################################################
# Name: client-rpc.sasl.enabled
# Required: No
# Default = false
#
# Specifies whether you want the RPC traffic to rocana-search to do SASL
# authentication using Kerberos. In a non-Kerberos environment, this setting
# should always be false (or commented out).  In a Kerberos environment, you are
# allowed to set it to either true or false.  When set to false (in a Kerberos
# environment), HDFS access will use Kerberos authentication, but the RPC queries
# to rocana-search will not.
#
# Ex.
# client-rpc.sasl.enabled = true
###################################################################################################
# Name: client-rpc.sasl.authorized-users
# Required: No
# Default: rocana
#
# If running in a kerberos secured environment and client-rpc.sasl.enabled=true,
# then you can specify what users are authorized to make RPC queries to rocana-search
# as a comma-separated list.
#
# Ex.
# client-rpc.sasl.authorized-users = rocana,user1,user2
###################################################################################################
# Name: hdfs.replication
# Required: No
# Default: 3
#
# HDFS replication used for Lucene indexes.
#
# Ex.
# hdfs.replication = 3
###################################################################################################
# Name: hdfs.root
# Required: Yes
#
# The absolute path of the root directory (with schema, hostname and port) where Rocana Search
# data should be stored. Collections will be placed in nested directories under this point.
#
# Ex.
# hdfs.root = hdfs://cdh:8020/rocana-search
###################################################################################################
# Name: hdfs.blockcache.enabled
# Required: No
# Default: true
#
# Whether the HDFS off-heap blockcache is enabled or not. This helps query performance
# by caching Lucene index blocks in the JVM (off-heap). The default is true.
#
# Ex.
# hdfs.blockcache.enabled = true
###################################################################################################
# Name: hdfs.blockcache.slab-size-mb
# Required: No
# Default: 128
#
# The size of HDFS blockcache slabs in megabytes. Default is 128.
#
# Ex.
# hdfs.blockcache.slab-size-mb = 128
###################################################################################################
# Name: hdfs.blockcache.slab-count
# Required: No
# Default: 1
#
# The number of HDFS blockcache slabs to create. Total blockcache memory can be calculated
# by multiplying slab-size-mb by slab-count. Default is 1, but this should be tuned based on
# the amount of system memory you have available.
#
# Ex.
# hdfs.blockcache.slab.count = 32
###################################################################################################
# Name: zookeeper.quorum
# Required: Yes
#
# A comma separated list of <hostname>:<port> of zookeeper servers where the
# Rocana Search will store its metadata, and where it can find the Kafka Broker
# information. Rocana Search metadata will be stored in under a znode specified by
# the zookeeper.metadata.root option below.
#
# Ex.
# zookeeper.quorum = cdh:2181,cdh-2:2181
###################################################################################################
# Name: zookeeper.metadata.root
# Required: No
# Default: /rocana-search
#
# Name of the znode (directory) in zookeeper where Rocana Search metadata will be stored.
#
# Ex.
# zookeeper.metadata.root = /rocana-search
###################################################################################################
# Name: kafka.brokers
# Required: Yes
#
# A comma separated list of <hostname>:<port> of kafka brokers, used for Rocana Search
# metadata messages.
#
# Ex.
# kafka.brokers = cdh:9092,cdh2:9092
###################################################################################################
# Name: kafka.metadata.topic
# Required: No
# Default: rocana-search-metadata
#
# The name of the Rocana Search metadata topic, defaults to rocana-search-metadata.
#
# Ex.
# kafka.metadata.topic = rocana-search-metadata
###################################################################################################
# Name: bind-address
# Required: No
# Default: 0.0.0.0
#
# The IP address the Rocana Search services (Query Coordinator and Query Executor) should listen on.
#
# Ex.
# bind-address = 10.10.1.106
###################################################################################################
# Name: coordinator.port
# Required: Yes
#
# The port the Rocana Search coordinator should listen on.
#
# Ex.
# coordinator.port = 65000
###################################################################################################
# Name: executor.port
# Required: Yes
#
# The port the Rocana Search query executor should listen on.
#
# Ex.
# executor.port = 65001
###################################################################################################
# Name: writers.expire-after-ms
# Required: No
# Default: 60000
#
# How long to wait after receiving the last event for a partition before the writer
# for that partition is closed. This is used to reduce resource usage as partitions
# become "old" and are no longer written to.
#
# Ex.
# writers.expire-after-ms = 60000
###################################################################################################
# Name: writers.ram-buffer-size-mb
# Required: No
# Default: 256
#
# Amount of memory to allow each writer to use before it flushes documents out to HDFS.
#
# Ex.
# writers.ram-buffer-size-mb = 256
###################################################################################################
# Name: searchers.refresh-after-ms
# Required: No
# Default: 15000
#
# How often Lucene searchers are refreshed. New documents are only visible after a search refresh,
# but refreshing searchers also uses CPU and disk I/O.
#
# Ex.
# searchers.refresh-after-ms = 20000
###################################################################################################
# Name: searchers.max-allowed
# Required: No
# Default: 20
#
# The maximum number of searchers kept in the searcher cache at once. Re-use of searchers makes
# followup searches faster, but open searchers use up memory. A separate searcher is required
# for each slice of a partition.
#
# Ex.
# searchers.max-allowed = 25
###################################################################################################
# Name: pipeline
# Required: Yes
#
# A comma separated list of pipeline configuration names. Each pipeline
# configuration creates one or more pipelines in the consumer based on the
# specified number of threads. Each pipeline is fully independent, with its own
# stream from Kafka, transformation engine, and underlying Lucene index writers.
#
# This property only declares the pipeline configuration name. Once declared,
# the pipeline configuration properties must be specified. See later in this
# file for an example.
#
# Ex. A single pipeline called 'events'.
# pipeline = events
#
# Ex. Two pipelines called 'events-a' and 'events-b'
# pipeline = events-a,events-b
###################################################################################################
# Name: pipeline.<pipeline>.kafka.zookeeper.quorum
# Required: Yes
#
# For each pipeline configuration named in the `pipeline` property, a set of properties
# using that pipeline name should be specified. Required properties are indicated as such.
# Each property in a pipeline configuration is prefixed by the name of the
# pipeline configuration. In this sample, we're creating an `events` pipeline
# configuration.
#
# A comma separated list of <hostname>:<port> of zookeeper servers where the
# Kafka brokers are registered. It's also possible to specify an optional root
# path in ZooKeeper after the port. This is useful when you're using a shared
# ZooKeeper quorum with other services.
#
# Ex.
# pipeline.events.kafka.zookeeper.quorum = zk1.rocana.com:2181,zk2.rocana.com:2181,zk3.rocana.com:2181/rocana
###################################################################################################
# Name: pipeline.<pipeline>.topic
# Required: Yes
#
# The name of the Kafka topic from which to consume events.
#
# Ex.
# pipeline.events.topic = events
###################################################################################################
# Name: pipeline.<pipeline>.dataset
# Required: Yes
#
# The name of the dataset to which events in this pipeline should be written.
#
# Ex.
# pipeline.events.dataset = events
###################################################################################################
# Name: pipeline.<pipeline>.partition-strategy-file
# Required: Yes
#
# Path to the search partition strategy file. This file should be in JSON format, the available
# partitioning types are "hash", "year-month-day", and "year-month-day-hour".
# Paths are relative to $ROCANA_HOME.
#
# All partitioners require a "source", which names the field (in the event schema) that
# is partitioned on.
#
# The "hash" partitioner also requires a "buckets" field, which is a positive integer
# identifying how many buckets to hash the "source" field into.
#
# An example partition strategy file:
# [
#   {"type": "year-month-day", "source": "ts"                    },
#   {"type": "hash",           "source": "location", "buckets": 4}
# ]
#
# If more than one partitioner is used (as in the above example), they will be applied
# in the order specified.  So the above partition strategy will result in directory
# paths like ${hdfs.root}/year=2015/month=12/day=05/location=2.
# See the Rocana Ops Reference Guide for more information about rocana-search partition strategies.
#
# Ex.
# pipeline.events.partition-strategy-file = conf/rocana-search/partition-strategy.json
###################################################################################################
# Name: pipeline.<pipeline>.consumer.group.id
# Required: Yes
#
# The group name of this consumer. When multiple consumers are part of the same
# group, they split up the work of taking the data. This gives you greater
# parallelism, but means there's no global ordering of events.
#
# Ex.
# pipeline.events.consumer.group.id = search-consumer
###################################################################################################
# Name: pipeline.<pipeline>.key.cache.size
# Required: No
# Default: 8192
#
# The minimum number of Kafka message keys per topic-partition to cache. If a cached key is seen
# again in the same partition the message is a duplicate, and it isn't processed. A value of 0
# disables de-duplication and stores all messages, regardless of their key.
#
# Ex.
# pipeline.events.key.cache.size = 4096
###################################################################################################
# Name: pipeline.<pipeline>.threads
# Required: Yes
# Default: 1
#
# The number of pipelines created with this configuration. For each thread there
# will be a separate Kafka stream and Lucene index writers.
#
# Only a single thread is allowed for Rocana Search writer pipelines.
#
# Ex.
# pipeline.events.threads = 1
###################################################################################################
# Name: pipeline.<pipeline>.writer.batch-time-ms
# Required: No
#
# The frequency with which received events are written to indexes, in milliseconds.
# The recommended lower bound and default is 1s (1000). Note that Lucene commits
# are based on commit time (below), not batches.
#
# Ex.
# pipeline.events.writer.batch-time-ms = 1000
###################################################################################################
# Name: pipeline.<pipeline>.writer.batch-size
# Required: No
#
# The maximum number of events the consumer will consume before writing the batch
# of events to an index. The recommended lower bound and default is 10000. Note that
# Lucene commits are based on commit time (below), not batches.
#
# Ex.
# pipeline.events.writer.batch-size = 10000
###################################################################################################
# Name: pipeline.<pipeline>.writer.commit-time-ms
# Required: No
# Default: 600000 (10 min)
#
# The frequency with which the Lucene indexes are hard committed, in milliseconds.
#
# Ex.
# pipeline.events.writer.commit-time-ms = 300000
###################################################################################################
# Name: pipeline.<pipeline>.writer.drop-event-types
# Required: No
#
# A comma seperated list of event type IDs that should be dropped by the writer.
# Defaults to "107" (host metrics). To keep all event types, assign this to an empty value.
#
# Ex.
# pipeline.events.writer.drop-event-types = 107
###################################################################################################
# Name: pipeline.<pipeline>.dead-letter.mode
# Required: Yes
#
# Dead letter mode determines what the system will do with improperly formatted events. The possible
# values are "queue" or "drop". If "queue" is selected, then you must also specify the dead letter
# topic, kafka brokers, and serializer class. Improperly formatted events will be placed on
# the specified kafka topic.
#
# If "drop" is specified, the improperly formatted events will just be dropped.
#
# Ex.
# pipeline.events.dead-letter.mode = drop
###################################################################################################
# Name: pipeline.<pipeline>.dead-letter.topic
# Required: Yes (if dead letter mode == queue)
#
# If dead letter mode has been set to "queue", then this will specify which kafka topic these
# events should be placed on.
#
# Ex.
# pipeline.events.dead-letter.topic = bad_events
###################################################################################################
# Name: pipeline.<pipeline>.dead-letter.producer.kafka.brokers
# Required: Yes (if dead letter mode == queue)
#
# A comma separated list of <hostname>:<port> of kafka brokers to use.
#
# Ex.
# pipeline.events.dead-letter.producer.kafka.brokers = kafka1.rocana.com:9092,kafka2.rocana.com:9092
###################################################################################################
# Name: pipeline.<pipeline>.dead-letter.producer.serializer.class
# Required: Yes (if dead letter mode == queue)
#
# You must configure how the events are to be encoded.
#
# Ex.
# pipeline.events.dead-letter.producer.serializer.class = kafka.serializer.DefaultEncoder
###################################################################################################
# Name: metrics.log-report-frequency
# Required: Yes
#
# The frequency at which metrics are output to the consumer logs, in seconds. If this
# value is 0, then there will not be any metrics output to the logs.
#
# Ex.
# metrics.log-report-frequency = 300
###################################################################################################
# Name: admin.server.bind-address
# Required: No
# Default: 0.0.0.0
#
# The IP or hostname to which the admin HTTP service should bind. If set to
# 0.0.0.0 (wildcard), the server will to all available interfaces.
#
# Ex.
# admin.server.bind-address = localhost
#
# Ex.
# admin.server.bind-address = consumer1234.rocana.com
###################################################################################################
# Name: admin.server.port
# Required: Yes
# Default: 17318
#
# The port to which the admin HTTP service should bind.
#
# Ex.
# admin.server.port = 17318
###################################################################################################
